<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LLxprt Code Core | LLxprt Code Docs</title>
  <link rel="stylesheet" href="../../../vybestack.css" />
</head>
<body>

  <nav>
    <div class="nav-container">
      <div class="nav-left">
        <a href="/" class="logo">
          <img src="/assets/vybestack_logo.png" alt="Vybestack" />
        </a>
        <span class="tagline">Beyond Vibe Coding</span>
      </div>
      <div class="nav-right">
        <div class="nav-dropdown">
          <a href="/llxprt-code.html">LLxprt Code</a>
          <div class="nav-dropdown-menu">
            <a href="/llxprt-code.html">Overview</a>
            <a href="/llxprt-code/docs/">Documentation</a>
          </div>
        </div>
        <a href="/jefe.html">LLxprt Jefe</a>
        <a href="/blog/">Blog</a>
        <a href="/#podcast">Podcast</a>
        <a href="https://discord.gg/Wc6dZqWWYv" target="_blank">Discord</a>
      </div>
    </div>
  </nav>


  <section class="section docs-section">
    <div class="container-wide">
      <div class="docs-layout">

      <nav class="docs-sidebar">
        <h3><a href="/llxprt-code/docs/">Documentation</a></h3>
        <ul>
          <li><a href="/llxprt-code/docs/getting-started.html">Getting Started Guide</a></li>
          <li><a href="/llxprt-code/docs/cli/providers.html">Provider Configuration</a></li>
          <li><a href="/llxprt-code/docs/cli/authentication.html">Authentication</a></li>
          <li><a href="/llxprt-code/docs/cli/profiles.html">Profiles</a></li>
          <li><a href="/llxprt-code/docs/sandbox.html">Sandboxing</a></li>
          <li><a href="/llxprt-code/docs/subagents.html">Subagents</a></li>
          <li><a href="/llxprt-code/docs/oauth-setup.html">OAuth Setup</a></li>
          <li><a href="/llxprt-code/docs/local-models.html">Local Models</a></li>
          <li><a href="/llxprt-code/docs/zed-integration.html">Zed Editor Integration</a></li>
          <li><a href="/llxprt-code/docs/cli/providers-openai-responses.html">OpenAI Responses API</a></li>
          <li><a href="/llxprt-code/docs/prompt-configuration.html">Prompt Configuration</a></li>
          <li><a href="/llxprt-code/docs/settings-and-profiles.html">Settings and Profiles</a></li>
          <li><a href="/llxprt-code/docs/checkpointing.html">Checkpointing</a></li>
          <li><a href="/llxprt-code/docs/extension.html">Extensions</a></li>
          <li><a href="/llxprt-code/docs/ide-integration.html">IDE Integration</a></li>
          <li><a href="/llxprt-code/docs/cli/configuration.html">Configuration</a></li>
          <li><a href="/llxprt-code/docs/cli/commands.html">Commands Reference</a></li>
          <li><a href="/llxprt-code/docs/troubleshooting.html">Troubleshooting Guide</a></li>
          <li><a href="/llxprt-code/docs/cli/index.html">CLI Introduction</a></li>
          <li><a href="/llxprt-code/docs/deployment.html">Execution and Deployment</a></li>
          <li><a href="/llxprt-code/docs/keyboard-shortcuts.html">Keyboard Shortcuts</a></li>
          <li><a href="/llxprt-code/docs/cli/themes.html">Themes</a></li>
          <li><a href="/llxprt-code/docs/EMOJI-FILTER.html">Emoji Filter</a></li>
          <li><a href="/llxprt-code/docs/cli/runtime-helpers.html">Runtime helper APIs</a></li>
          <li><a href="/llxprt-code/docs/cli/context-dumping.html">Context Dumping</a></li>
          <li><a href="/llxprt-code/docs/telemetry.html">Telemetry</a></li>
          <li><a href="/llxprt-code/docs/telemetry-privacy.html">Telemetry Privacy</a></li>
          <li><a href="/llxprt-code/docs/gemini-cli-tips.html">Migration from Gemini CLI</a></li>
          <li><a href="/llxprt-code/docs/architecture.html">Architecture Overview</a></li>
          <li><a href="/llxprt-code/docs/core/index.html">Core Introduction</a></li>
          <li><a href="/llxprt-code/docs/core/provider-runtime-context.html">Provider runtime context</a></li>
          <li><a href="/llxprt-code/docs/core/provider-interface.html">Provider interface</a></li>
          <li><a href="/llxprt-code/docs/core/tools-api.html">Tools API</a></li>
          <li><a href="/llxprt-code/docs/core/memport.html">Memory Import Processor</a></li>
          <li><a href="/llxprt-code/docs/shell-replacement.html">Shell Replacement</a></li>
          <li><a href="/llxprt-code/docs/../CONTRIBUTING.html">Contributing & Development Guide</a></li>
          <li><a href="/llxprt-code/docs/npm.html">NPM Workspaces and Publishing</a></li>
          <li><a href="/llxprt-code/docs/migration/stateless-provider.html">Stateless provider migration</a></li>
          <li><a href="/llxprt-code/docs/tools/index.html">Tools Overview</a></li>
          <li><a href="/llxprt-code/docs/tools/file-system.html">File System Tools</a></li>
          <li><a href="/llxprt-code/docs/tools/multi-file.html">Multi-File Read Tool</a></li>
          <li><a href="/llxprt-code/docs/tools/shell.html">Shell Tool</a></li>
          <li><a href="/llxprt-code/docs/tools/mcp-server.html">MCP Server</a></li>
          <li><a href="/llxprt-code/docs/tools/web-fetch.html">Web Fetch Tool</a></li>
          <li><a href="/llxprt-code/docs/tools/web-search.html">Web Search Tool</a></li>
          <li><a href="/llxprt-code/docs/tools/memory.html">Memory Tool</a></li>
          <li><a href="/llxprt-code/docs/release-notes/stateless-provider.html">Release notes: Stateless Provider</a></li>
          <li><a href="/llxprt-code/docs/tos-privacy.html">Terms of Service and Privacy Notice</a></li>
        </ul>
      </nav>
        <div class="docs-content">
          <div class="blog-post-content">
            <h1>LLxprt Code Core</h1>
<p>LLxprt Code's core package (<code>packages/core</code>) is the backend portion of LLxprt Code, handling communication with multiple AI providers (Google Gemini, OpenAI, Anthropic, and others), managing tools, and processing requests sent from <code>packages/cli</code>. For a general overview of LLxprt Code, see the <a href="../index.html">main documentation page</a>.</p>
<h2>Navigating this section</h2>
<ul>
<li><strong><a href="./provider-runtime-context.html">Provider runtime context</a>:</strong> Details on <code>ProviderRuntimeContext</code> lifecycle helpers and runtime isolation semantics.</li>
<li><strong><a href="./provider-interface.html">Provider interface</a>:</strong> API reference for implementing providers against the stateless runtime.</li>
<li><strong><a href="./tools-api.html">Core tools API</a>:</strong> Information on how tools are defined, registered, and used by the core.</li>
<li><strong><a href="./memport.html">Memory Import Processor</a>:</strong> Documentation for the modular <a href="http://LLXPRT.md">LLXPRT.md</a> import feature using @file.md syntax.</li>
<li><strong><a href="./policy-engine.html">Policy Engine</a>:</strong> Use the Policy Engine for fine-grained control over tool execution.</li>
</ul>
<h2>Role of the core</h2>
<p>While the <code>packages/cli</code> portion of LLxprt Code provides the user interface, <code>packages/core</code> is responsible for:</p>
<ul>
<li><strong>AI Provider interaction:</strong> Securely communicating with various AI providers (Google Gemini, OpenAI, Anthropic, etc.), sending user prompts, and receiving model responses.</li>
<li><strong>Prompt engineering:</strong> Constructing effective prompts for different AI models, potentially incorporating conversation history, tool definitions, and instructional context from <code>LLXPRT.md</code> files.</li>
<li><strong>Tool management &amp; orchestration:</strong>
<ul>
<li>Registering available tools (e.g., file system tools, shell command execution).</li>
<li>Interpreting tool use requests from the AI model.</li>
<li>Executing the requested tools with the provided arguments.</li>
<li>Returning tool execution results to the AI model for further processing.</li>
</ul>
</li>
<li><strong>Session and state management:</strong> Keeping track of the conversation state, including history and any relevant context required for coherent interactions.</li>
<li><strong>Configuration:</strong> Managing core-specific configurations, such as API key access, model selection, provider settings, and tool settings.</li>
</ul>
<h2>Security considerations</h2>
<p>The core plays a vital role in security:</p>
<ul>
<li><strong>API key management:</strong> It handles various API keys (<code>GEMINI_API_KEY</code>, <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, etc.) and ensures they're used securely when communicating with their respective providers.</li>
<li><strong>Tool execution:</strong> When tools interact with the local system (e.g., <code>run_shell_command</code>), the core (and its underlying tool implementations) must do so with appropriate caution, often involving sandboxing mechanisms to prevent unintended modifications.</li>
</ul>
<h2>Chat history compression</h2>
<p>To ensure that long conversations don't exceed the token limits of the AI model, the core includes a chat history compression feature.</p>
<p>When a conversation approaches the token limit for the configured model, the core automatically compresses the conversation history before sending it to the model. This compression is designed to be lossless in terms of the information conveyed, but it reduces the overall number of tokens used.</p>
<p>Token limits vary by provider and model:</p>
<ul>
<li><strong>Google Gemini:</strong> See the <a href="https://ai.google.dev/gemini-api/docs/models">Google AI documentation</a></li>
<li><strong>OpenAI:</strong> Models like GPT-4.1 and o3 have different context windows</li>
<li><strong>Anthropic:</strong> Claude models offer various context window sizes</li>
</ul>
<h2>Model fallback (Disabled in LLxprt)</h2>
<p><strong>Note:</strong> LLxprt Code has disabled automatic model fallback. When you select a model, it will stay on that model throughout your session. This prevents unexpected model changes mid-conversation (e.g., switching from a powerful model to a less capable one while coding).</p>
<p>The upstream Gemini CLI includes an automatic fallback mechanism that switches from &quot;pro&quot; to &quot;flash&quot; models when rate-limited. LLxprt intentionally disables this behavior to maintain consistency in your AI interactions.</p>
<p>If you encounter rate limits, you can manually switch models using the <code>/model</code> command or wait for the rate limit to reset. Other providers may have their own rate limiting behaviors - consult their documentation for details.</p>
<h2>File discovery service</h2>
<p>The file discovery service is responsible for finding files in the project that are relevant to the current context. It is used by the <code>@</code> command and other tools that need to access files.</p>
<h2>Memory discovery service</h2>
<p>The memory discovery service is responsible for finding and loading the <code>LLXPRT.md</code> files that provide context to the model. It searches for these files in a hierarchical manner, starting from the current working directory and moving up to the project root and the user's home directory. It also searches in subdirectories.</p>
<p>This allows you to have global, project-level, and component-level context files, which are all combined to provide the model with the most relevant information.</p>
<p>You can use the <a href="../cli/commands.html"><code>/memory</code> command</a> to <code>show</code>, <code>add</code>, and <code>refresh</code> the content of loaded <code>LLXPRT.md</code> files.</p>
<h2>Citations</h2>
<p>When the AI model finds it is reciting text from a source it appends the citation to the output. It is disabled by default but can be enabled with the ui.showCitations setting.</p>
<ul>
<li>When proposing an edit the citations display before giving the user the option to accept.</li>
<li>Citations are always shown at the end of the model's turn.</li>
<li>We deduplicate citations and display them in alphabetical order.</li>
</ul>

          </div>
        </div>
      </div>
    </div>
  </section>


  <footer class="footer">
    <div class="footer-container">
      <div class="footer-section">
        <h4>Vybestack</h4>
        <p>Beyond vibe coding. Autonomous development for ascending engineers.</p>
      </div>
      <div class="footer-section">
        <h4>Products</h4>
        <ul>
          <li><a href="/llxprt-code.html">LLxprt Code</a></li>
          <li><a href="/jefe.html">LLxprt Jefe</a></li>
        </ul>
      </div>
      <div class="footer-section">
        <h4>Content</h4>
        <ul>
          <li><a href="/blog/">Blog</a></li>
          <li><a href="/#podcast">Podcast</a></li>
          <li><a href="/llxprt-code/docs/">Documentation</a></li>
        </ul>
      </div>
      <div class="footer-section">
        <h4>Connect</h4>
        <ul class="social-links">
          <li><a href="https://github.com/vybestack/llxprt-code"><img src="/assets/github-mark-white.svg" alt="GitHub" /> </a></li>
          <li><a href="https://discord.gg/Wc6dZqWWYv"><img src="/assets/discord-mark-white.svg" alt="Discord" /></a></li>
          <li><a href="https://www.linkedin.com/company/vybestack/"><img src="/assets/linkedin-white.svg" alt="LinkedIn" /></a></li>
        </ul>
      </div>
    </div>
    <div class="footer-bottom">
      <p>&copy; 2026 Vybestack. Apache 2.0 License. Built for the terminal.</p>
    </div>
  </footer>

</body>
</html>